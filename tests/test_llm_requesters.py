#!/usr/bin/env python3
"""
LLMè¯·æ±‚å™¨å®Œæ•´æµ‹è¯•è„šæœ¬ - æµ‹è¯•æ‰€æœ‰LLMè¯·æ±‚å™¨çš„åŠŸèƒ½
"""

import os
import sys
import asyncio
import json
from typing import Dict, Any, List
from unittest.mock import Mock, patch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from src.base.Base import Base
from src.modules.LLMRequester import (
    LLMRequester,
    OpenaiRequester,
    LocalLLMRequester,
    AnthropicRequester,
    CohereRequester,
    GoogleRequester,
    AmazonbedrockRequester,
    SakuraRequester,
    DashscopeRequester,
    LLMClientFactory,
    get_default_config,
    get_supported_platforms
)


class LLMRequesterTester(Base):
    """LLMè¯·æ±‚å™¨æµ‹è¯•ç±»"""

    def __init__(self):
        # è®¾ç½®é»˜è®¤é…ç½® - ç®€åŒ–ç»“æ„
        self.default = {}

        # ç›´æ¥è®¾ç½®æµ‹è¯•é…ç½®
        self.test_configs = {
            "openai": {
                "target_platform": "openai",
                "api_key": "test_key",
                "api_url": "https://api.openai.com/v1",
                "model_name": "gpt-4o-mini",
                "temperature": 0.7,
                "request_timeout": 30
            },
            "local": {
                "target_platform": "LocalLLM",
                "api_key": "none_api_key",
                "api_url": "http://localhost:8000/v1",
                "model_name": "deepseek-r1:7b",
                "temperature": 0.7,
                "request_timeout": 30,
                "think_switch": True
            },
            "anthropic": {
                "target_platform": "anthropic",
                "api_key": "test_key",
                "api_url": "https://api.anthropic.com",
                "model_name": "claude-3-5-sonnet-20241022",
                "temperature": 0.7,
                "request_timeout": 30
            }
        }

        super().__init__()

        # æµ‹è¯•æ•°æ®
        self.test_messages = [
            {"role": "user", "content": "Hello, please respond with 'Test successful' if you can see this message."}
        ]

        self.c_to_rust_messages = [
            {
                "role": "user",
                "content": """Convert this simple C code to Rust:

```c
#include <stdio.h>

int add(int a, int b) {
    return a + b;
}

int main() {
    int result = add(3, 4);
    printf("Result: %d\\n", result);
    return 0;
}
```"""
            }
        ]

        self.system_prompt = "You are a helpful assistant for testing C to Rust conversion."
        self.c_to_rust_prompt = """You are an expert in converting C code to Rust.
Convert the provided C code to safe, idiomatic Rust code.
Focus on memory safety and idiomatic Rust patterns."""

    def test_supported_platforms(self):
        """æµ‹è¯•æ”¯æŒçš„å¹³å°åˆ—è¡¨"""
        self.info("=" * 50)
        self.info("æµ‹è¯•æ”¯æŒçš„å¹³å°åˆ—è¡¨")
        self.info("=" * 50)

        try:
            platforms = get_supported_platforms()
            self.info(f"æ”¯æŒçš„å¹³å°æ•°é‡: {len(platforms)}")

            for i, platform in enumerate(platforms, 1):
                self.info(f"  {i}. {platform}")

                # æµ‹è¯•é»˜è®¤é…ç½®
                default_config = get_default_config(platform)
                if default_config:
                    self.debug(f"    é»˜è®¤é…ç½®: {json.dumps(default_config, indent=2, ensure_ascii=False)}")
                else:
                    self.warning(f"    å¹³å° {platform} æ²¡æœ‰é»˜è®¤é…ç½®")

            self.info("âœ“ æ”¯æŒå¹³å°åˆ—è¡¨æµ‹è¯•é€šè¿‡")
            return True

        except Exception as e:
            self.error(f"âœ— æ”¯æŒå¹³å°åˆ—è¡¨æµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_llm_client_factory(self):
        """æµ‹è¯•LLMå®¢æˆ·ç«¯å·¥å‚"""
        self.info("=" * 50)
        self.info("æµ‹è¯•LLMå®¢æˆ·ç«¯å·¥å‚")
        self.info("=" * 50)

        try:
            factory = LLMClientFactory()
            self.info("âœ“ LLMå®¢æˆ·ç«¯å·¥å‚åˆ›å»ºæˆåŠŸ")

            # æµ‹è¯•å„ç§å®¢æˆ·ç«¯åˆ›å»º
            test_configs = [
                ("openai", self.test_configs["openai"]),
                ("local", self.test_configs["local"]),
                ("anthropic", self.test_configs["anthropic"])
            ]

            success_count = 0
            for client_type, config in test_configs:
                try:
                    if client_type == "openai":
                        client = factory.get_openai_client(config)
                    elif client_type == "local":
                        client = factory.get_openai_client_local(config)
                    elif client_type == "anthropic":
                        client = factory.get_anthropic_client(config)

                    if client:
                        self.info(f"âœ“ {client_type} å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
                        success_count += 1
                    else:
                        self.error(f"âœ— {client_type} å®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥")

                except Exception as e:
                    self.error(f"âœ— {client_type} å®¢æˆ·ç«¯åˆ›å»ºå¼‚å¸¸: {e}")

            self.info(f"å®¢æˆ·ç«¯å·¥å‚æµ‹è¯•ç»“æœ: {success_count}/{len(test_configs)} æˆåŠŸ")
            return success_count > 0

        except Exception as e:
            self.error(f"âœ— LLMå®¢æˆ·ç«¯å·¥å‚æµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_individual_requesters(self):
        """æµ‹è¯•å„ä¸ªè¯·æ±‚å™¨çš„åˆå§‹åŒ–"""
        self.info("=" * 50)
        self.info("æµ‹è¯•å„ä¸ªè¯·æ±‚å™¨åˆå§‹åŒ–")
        self.info("=" * 50)

        requesters = [
            ("OpenaiRequester", OpenaiRequester),
            ("LocalLLMRequester", LocalLLMRequester),
            ("AnthropicRequester", AnthropicRequester),
            ("CohereRequester", CohereRequester),
            ("GoogleRequester", GoogleRequester),
            ("AmazonbedrockRequester", AmazonbedrockRequester),
            ("SakuraRequester", SakuraRequester),
            ("DashscopeRequester", DashscopeRequester)
        ]

        success_count = 0
        for name, requester_class in requesters:
            try:
                requester = requester_class()
                if requester:
                    self.info(f"âœ“ {name} åˆå§‹åŒ–æˆåŠŸ")
                    success_count += 1
                else:
                    self.error(f"âœ— {name} åˆå§‹åŒ–å¤±è´¥")

            except Exception as e:
                self.error(f"âœ— {name} åˆå§‹åŒ–å¼‚å¸¸: {e}")

        self.info(f"è¯·æ±‚å™¨åˆå§‹åŒ–æµ‹è¯•ç»“æœ: {success_count}/{len(requesters)} æˆåŠŸ")
        return success_count == len(requesters)

    def test_main_requester(self):
        """æµ‹è¯•ä¸»è¯·æ±‚å™¨"""
        self.info("=" * 50)
        self.info("æµ‹è¯•ä¸»è¯·æ±‚å™¨")
        self.info("=" * 50)

        try:
            requester = LLMRequester()
            self.info("âœ“ ä¸»è¯·æ±‚å™¨åˆ›å»ºæˆåŠŸ")

            # æµ‹è¯•é…ç½®éªŒè¯
            test_configs = [
                ("æœ‰æ•ˆé…ç½®", self.test_configs["local"]),
                ("ç¼ºå°‘å¹³å°é…ç½®", {"model_name": "test"}),
                ("ç©ºé…ç½®", {})
            ]

            for test_name, config in test_configs:
                is_valid = requester.validate_config(config)
                self.info(f"  {test_name}: {'âœ“ æœ‰æ•ˆ' if is_valid else 'âœ— æ— æ•ˆ'}")

            # æµ‹è¯•æ”¯æŒå¹³å°åˆ—è¡¨
            platforms = requester.get_supported_platforms()
            self.info(f"âœ“ æ”¯æŒå¹³å°æ•°é‡: {len(platforms)}")

            return True

        except Exception as e:
            self.error(f"âœ— ä¸»è¯·æ±‚å™¨æµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_mock_requests(self):
        """ä½¿ç”¨Mockæµ‹è¯•è¯·æ±‚åŠŸèƒ½"""
        self.info("=" * 50)
        self.info("æµ‹è¯•Mockè¯·æ±‚")
        self.info("=" * 50)

        try:
            # åˆ›å»ºMockå“åº”
            mock_response = Mock()
            mock_response.choices = [Mock()]
            mock_response.choices[0].message = Mock()
            mock_response.choices[0].message.content = "Test successful - this is a mock response"
            mock_response.usage = Mock()
            mock_response.usage.prompt_tokens = 10
            mock_response.usage.completion_tokens = 8

            # æµ‹è¯•OpenAIè¯·æ±‚å™¨
            with patch('src.modules.LLMRequester.LLMClientFactory.LLMClientFactory') as mock_factory:
                mock_client = Mock()
                mock_client.chat.completions.create.return_value = mock_response
                mock_factory.return_value.get_openai_client_local.return_value = mock_client

                requester = LocalLLMRequester()
                config = self.config["llm"]["providers"]["LocalLLM"]

                error, think, content, prompt_tokens, completion_tokens = requester.request_LocalLLM(
                    self.test_messages, self.system_prompt, config
                )

                if not error and content:
                    self.info("âœ“ Mock LocalLLMè¯·æ±‚æˆåŠŸ")
                    self.info(f"  å“åº”å†…å®¹: {content}")
                    self.info(f"  Tokenæ¶ˆè€—: {prompt_tokens} è¾“å…¥, {completion_tokens} è¾“å‡º")
                    return True
                else:
                    self.error("âœ— Mock LocalLLMè¯·æ±‚å¤±è´¥")
                    return False

        except Exception as e:
            self.error(f"âœ— Mockè¯·æ±‚æµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_real_local_request(self):
        """æµ‹è¯•çœŸå®çš„æœ¬åœ°LLMè¯·æ±‚ï¼ˆå¦‚æœæœåŠ¡å¯ç”¨ï¼‰"""
        self.info("=" * 50)
        self.info("æµ‹è¯•çœŸå®æœ¬åœ°LLMè¯·æ±‚")
        self.info("=" * 50)

        try:
            requester = LocalLLMRequester()
            config = self.test_configs["local"]

            self.info("å‘é€ç®€å•æµ‹è¯•è¯·æ±‚...")
            self.info(f"API URL: {config['api_url']}")
            self.info(f"Model: {config['model_name']}")

            # å‘èµ·è¯·æ±‚
            error, think, content, prompt_tokens, completion_tokens = requester.request_LocalLLM(
                self.test_messages, self.system_prompt, config
            )

            if error:
                self.warning("æœ¬åœ°LLMæœåŠ¡ä¸å¯ç”¨ï¼Œè¿™æ˜¯æ­£å¸¸çš„ï¼ˆå¦‚æœæœåŠ¡æœªå¯åŠ¨ï¼‰")
                return True
            else:
                self.info("âœ“ æœ¬åœ°LLMè¯·æ±‚æˆåŠŸ")
                self.info(f"å“åº”å†…å®¹: {content}")
                if think:
                    self.info(f"æ¨ç†è¿‡ç¨‹: {think[:100]}...")
                self.info(f"Tokenæ¶ˆè€—: {prompt_tokens} è¾“å…¥, {completion_tokens} è¾“å‡º")
                return True

        except Exception as e:
            self.warning(f"æœ¬åœ°LLMè¯·æ±‚æµ‹è¯•å¼‚å¸¸: {e} (è¿™å¯èƒ½æ˜¯å› ä¸ºæœåŠ¡æœªå¯åŠ¨)")
            return True  # ä¸ç®—ä½œå¤±è´¥ï¼Œå› ä¸ºæœåŠ¡å¯èƒ½æœªå¯åŠ¨

    def test_c_to_rust_conversion_mock(self):
        """ä½¿ç”¨Mockæµ‹è¯•Cåˆ°Rustè½¬æ¢"""
        self.info("=" * 50)
        self.info("æµ‹è¯•Cåˆ°Rustè½¬æ¢ (Mock)")
        self.info("=" * 50)

        rust_code = '''fn add(a: i32, b: i32) -> i32 {
    a + b
}

fn main() {
    let result = add(3, 4);
    println!("Result: {}", result);
}'''

        try:
            # åˆ›å»ºMockå“åº”
            mock_response = Mock()
            mock_response.choices = [Mock()]
            mock_response.choices[0].message = Mock()
            mock_response.choices[0].message.content = f"Here's the Rust conversion:\n\n```rust\n{rust_code}\n```"
            mock_response.usage = Mock()
            mock_response.usage.prompt_tokens = 50
            mock_response.usage.completion_tokens = 30

            with patch('src.modules.LLMRequester.LLMClientFactory.LLMClientFactory') as mock_factory:
                mock_client = Mock()
                mock_client.chat.completions.create.return_value = mock_response
                mock_factory.return_value.get_openai_client_local.return_value = mock_client

                requester = LLMRequester()
                config = self.test_configs["local"]

                error, think, content, prompt_tokens, completion_tokens = requester.sent_request(
                    self.c_to_rust_messages, self.c_to_rust_prompt, config
                )

                if not error and content:
                    self.info("âœ“ Mock Cåˆ°Rustè½¬æ¢æˆåŠŸ")
                    self.info("è½¬æ¢ç»“æœ:")
                    self.info("-" * 40)
                    print(content)
                    self.info("-" * 40)
                    self.info(f"Tokenæ¶ˆè€—: {prompt_tokens} è¾“å…¥, {completion_tokens} è¾“å‡º")
                    return True
                else:
                    self.error("âœ— Mock Cåˆ°Rustè½¬æ¢å¤±è´¥")
                    return False

        except Exception as e:
            self.error(f"âœ— Mock Cåˆ°Rustè½¬æ¢æµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_error_handling(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        self.info("=" * 50)
        self.info("æµ‹è¯•é”™è¯¯å¤„ç†")
        self.info("=" * 50)

        try:
            requester = LLMRequester()

            # æµ‹è¯•æ— æ•ˆé…ç½®
            invalid_configs = [
                {"target_platform": "invalid_platform"},
                {"target_platform": "openai", "api_key": "", "api_url": "invalid_url"},
                {}
            ]

            error_handled_count = 0
            for i, config in enumerate(invalid_configs, 1):
                try:
                    error, think, content, prompt_tokens, completion_tokens = requester.sent_request(
                        self.test_messages, self.system_prompt, config
                    )

                    if error:
                        self.info(f"âœ“ é”™è¯¯é…ç½® {i} æ­£ç¡®å¤„ç†äº†é”™è¯¯")
                        error_handled_count += 1
                    else:
                        self.warning(f"âš  é”™è¯¯é…ç½® {i} æ²¡æœ‰è¿”å›é”™è¯¯ï¼ˆå¯èƒ½ä½¿ç”¨äº†é»˜è®¤å¤„ç†ï¼‰")

                except Exception as e:
                    self.info(f"âœ“ é”™è¯¯é…ç½® {i} æ­£ç¡®æŠ›å‡ºå¼‚å¸¸: {e}")
                    error_handled_count += 1

            self.info(f"é”™è¯¯å¤„ç†æµ‹è¯•ç»“æœ: {error_handled_count}/{len(invalid_configs)} æ­£ç¡®å¤„ç†")
            return error_handled_count > 0

        except Exception as e:
            self.error(f"âœ— é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
            return False

    def test_config_templates(self):
        """æµ‹è¯•é…ç½®æ¨¡æ¿"""
        self.info("=" * 50)
        self.info("æµ‹è¯•é…ç½®æ¨¡æ¿")
        self.info("=" * 50)

        try:
            platforms = get_supported_platforms()
            template_count = 0

            for platform in platforms:
                config = get_default_config(platform)
                if config:
                    template_count += 1
                    self.info(f"âœ“ {platform} æœ‰é…ç½®æ¨¡æ¿")

                    # éªŒè¯å¿…è¦å­—æ®µ
                    if "target_platform" in config:
                        self.debug(f"  target_platform: {config['target_platform']}")
                    else:
                        self.warning(f"  ç¼ºå°‘ target_platform å­—æ®µ")

                else:
                    self.warning(f"âš  {platform} æ²¡æœ‰é…ç½®æ¨¡æ¿")

            self.info(f"é…ç½®æ¨¡æ¿æµ‹è¯•ç»“æœ: {template_count}/{len(platforms)} æœ‰æ¨¡æ¿")
            return template_count > 0

        except Exception as e:
            self.error(f"âœ— é…ç½®æ¨¡æ¿æµ‹è¯•å¤±è´¥: {e}")
            return False

    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        self.info("å¼€å§‹LLMè¯·æ±‚å™¨å®Œæ•´æµ‹è¯•")
        self.info("=" * 60)

        tests = [
            ("æ”¯æŒå¹³å°åˆ—è¡¨æµ‹è¯•", self.test_supported_platforms),
            ("LLMå®¢æˆ·ç«¯å·¥å‚æµ‹è¯•", self.test_llm_client_factory),
            ("è¯·æ±‚å™¨åˆå§‹åŒ–æµ‹è¯•", self.test_individual_requesters),
            ("ä¸»è¯·æ±‚å™¨æµ‹è¯•", self.test_main_requester),
            ("Mockè¯·æ±‚æµ‹è¯•", self.test_mock_requests),
            ("é…ç½®æ¨¡æ¿æµ‹è¯•", self.test_config_templates),
            ("é”™è¯¯å¤„ç†æµ‹è¯•", self.test_error_handling),
            ("Cåˆ°Rustè½¬æ¢Mockæµ‹è¯•", self.test_c_to_rust_conversion_mock),
        ]

        # è¿è¡ŒåŸºç¡€æµ‹è¯•
        passed = 0
        for test_name, test_func in tests:
            try:
                if test_func():
                    passed += 1
                    self.info(f"âœ“ {test_name} é€šè¿‡")
                else:
                    self.error(f"âœ— {test_name} å¤±è´¥")
            except Exception as e:
                self.error(f"âœ— {test_name} å¼‚å¸¸: {e}")

            self.info("")

        # è¿è¡ŒçœŸå®è¯·æ±‚æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
        self.info("å¼€å§‹çœŸå®è¯·æ±‚æµ‹è¯•ï¼ˆå¯é€‰ï¼‰...")
        self.info("")

        optional_tests = [
            ("çœŸå®æœ¬åœ°LLMè¯·æ±‚æµ‹è¯•", self.test_real_local_request),
        ]

        for test_name, test_func in optional_tests:
            try:
                if test_func():
                    passed += 1
                    self.info(f"âœ“ {test_name} é€šè¿‡")
                else:
                    self.info(f"âš  {test_name} è·³è¿‡æˆ–å¤±è´¥ï¼ˆè¿™æ˜¯æ­£å¸¸çš„ï¼‰")
            except Exception as e:
                self.info(f"âš  {test_name} å¼‚å¸¸: {e} ï¼ˆè¿™æ˜¯æ­£å¸¸çš„ï¼‰")

            self.info("")

        # è¾“å‡ºæµ‹è¯•æ€»ç»“
        total_tests = len(tests) + len(optional_tests)
        self.info("=" * 60)
        self.info(f"æµ‹è¯•æ€»ç»“: {passed}/{total_tests} é€šè¿‡")

        if passed >= len(tests):
            self.info("âœ“ LLMè¯·æ±‚å™¨åŸºç¡€åŠŸèƒ½æ­£å¸¸")
            if passed == total_tests:
                self.info("âœ“ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ŒåŒ…æ‹¬çœŸå®æœåŠ¡æµ‹è¯•")
            else:
                self.info("âš  çœŸå®æœåŠ¡æµ‹è¯•æœªé€šè¿‡ï¼ˆå¦‚æœæœåŠ¡æœªå¯åŠ¨ï¼Œè¿™æ˜¯æ­£å¸¸çš„ï¼‰")
        else:
            self.error("âœ— LLMè¯·æ±‚å™¨å­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥å®ç°")

        return passed >= len(tests)

    def interactive_test_menu(self):
        """äº¤äº’å¼æµ‹è¯•èœå•"""
        self.info("=" * 60)
        self.info("LLMè¯·æ±‚å™¨äº¤äº’å¼æµ‹è¯•")
        self.info("=" * 60)

        while True:
            print("\nè¯·é€‰æ‹©æµ‹è¯•é¡¹ç›®:")
            print("1. è¿è¡Œæ‰€æœ‰æµ‹è¯•")
            print("2. æ”¯æŒå¹³å°åˆ—è¡¨æµ‹è¯•")
            print("3. å®¢æˆ·ç«¯å·¥å‚æµ‹è¯•")
            print("4. è¯·æ±‚å™¨åˆå§‹åŒ–æµ‹è¯•")
            print("5. Mockè¯·æ±‚æµ‹è¯•")
            print("6. çœŸå®æœ¬åœ°LLMè¯·æ±‚æµ‹è¯•")
            print("7. Cåˆ°Rustè½¬æ¢Mockæµ‹è¯•")
            print("8. é…ç½®æ¨¡æ¿æµ‹è¯•")
            print("9. é”™è¯¯å¤„ç†æµ‹è¯•")
            print("0. é€€å‡º")

            try:
                choice = input("\nè¯·è¾“å…¥é€‰æ‹© (0-9): ").strip()

                if choice == "0":
                    self.info("é€€å‡ºæµ‹è¯•")
                    break
                elif choice == "1":
                    self.run_all_tests()
                elif choice == "2":
                    self.test_supported_platforms()
                elif choice == "3":
                    self.test_llm_client_factory()
                elif choice == "4":
                    self.test_individual_requesters()
                elif choice == "5":
                    self.test_mock_requests()
                elif choice == "6":
                    self.test_real_local_request()
                elif choice == "7":
                    self.test_c_to_rust_conversion_mock()
                elif choice == "8":
                    self.test_config_templates()
                elif choice == "9":
                    self.test_error_handling()
                else:
                    self.warning("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")

            except KeyboardInterrupt:
                self.info("\n\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
                break
            except Exception as e:
                self.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("C2Rust Agent - LLMè¯·æ±‚å™¨æµ‹è¯•å·¥å…·")
    print("=" * 60)

    if len(sys.argv) > 1 and sys.argv[1] == "--interactive":
        # äº¤äº’å¼æ¨¡å¼
        try:
            tester = LLMRequesterTester()
            tester.interactive_test_menu()
        except KeyboardInterrupt:
            print("\n\nâš  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            sys.exit(1)
    else:
        # è‡ªåŠ¨æµ‹è¯•æ¨¡å¼
        try:
            tester = LLMRequesterTester()
            success = tester.run_all_tests()

            if success:
                print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼LLMè¯·æ±‚å™¨åŠŸèƒ½æ­£å¸¸ã€‚")
                print("\nğŸ’¡ æç¤º:")
                print("   - å¦‚æœçœŸå®è¯·æ±‚æµ‹è¯•å¤±è´¥ï¼Œè¯·ç¡®ä¿LLMæœåŠ¡å·²å¯åŠ¨")
                print("   - æœ¬åœ°æœåŠ¡é»˜è®¤åœ°å€: http://localhost:8000/v1")
                print("   - å¯ä»¥ä¿®æ”¹ config/config.json æ¥è°ƒæ•´é…ç½®")
                print("   - ä½¿ç”¨ --interactive å‚æ•°è¿›å…¥äº¤äº’å¼æµ‹è¯•æ¨¡å¼")
            else:
                print("\nâŒ æµ‹è¯•å¤±è´¥ï¼è¯·æ£€æŸ¥LLMè¯·æ±‚å™¨å®ç°ã€‚")
                sys.exit(1)

        except KeyboardInterrupt:
            print("\n\nâš  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            sys.exit(1)


if __name__ == "__main__":
    main()
